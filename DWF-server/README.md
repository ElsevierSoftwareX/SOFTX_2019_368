# Run docker container

### Use DockerHub image

    docker swarm init
    docker stack deploy -c docker-compose.yml dwf_stack

### Or build it from source

    docker build -t=dwf-server .
    docker tag dwf-server viszkoktamas93/dwf-server
    docker swarm init
    docker stack deploy -c docker-compose.yml dwf_stack

## Server is listening on port 4000

## Set up common Samba share for server and workers

At the moment, a Samba share can be used as a common storage space (where the feature data or input csvs can be shared between the workers and server). To set up your shared drive, use the following environment variables

    SMB_VOL=URL to your share
    SMB_DOMAIN=the domain name
    SMB_USER=your username
    SMB_PWD=your password
    
Once these variables are set, a new docker volume will be created and attached to the docker images, which can access these shared locations.

There are several starting scripts for the docker stack (batch and shell scripts), which will automatically ask these information from the user on start-up.

## Setting up default Kibana dashboard

The Deep-Water Framework comes with a default data visualization dashboard created with Kibana. To be able to use it, first its description needs to be imported into the underlying elasticsearch database. To import the dashboard open Kibana control panel after starting the dwf docker stack:

    http://localhost:5601/app/kibana#/management

Go to *Saved Objects*, select *Import*, and load the dashboard description file:

    DWF-server/templates/sample_dashboard.ndjson
    
Once it is loaded into Kibana, the *Results Dashboard* menu in the DWF server will open the dashboard visualizing the results of learnings.
    
# API endpoints

## `/ping - POST`

- description:
  - client sends request to this endpoint every 30s, signaling that they are ready to process an available task
  - when a client queries this endpoint for the first time, the server generates a unique hash that it can use for authentication later on

- request body:

      { "hash": "asd123" }

  - hash (str): hash generated by the server, empty in case of new client

- response:

  - success:

        {
            "hash": "asd123",
            "task": {...}
        }, status: 200

    - hash (str): the client's hash code
    - task (obj | ""):

      - object if the server assigned a task to the client
      - empty string if there is no task assigned to the client identified by the given hash

  - error:

        { "message": "Uknown worker id." }, status: 400

## `/status - POST`

- description:
  - client sends request to this endpoint after every subtask completed thus the server can track the progress of the task that the client currently works on

- request: 

      {
          "hash": "asd123",
          "progress": 0.42,
          "message": "DBH started..."
      }

  - hash (str): the client's hash code
  - progress (float): task completeness, between 0 and 1
  - message (str): any message to log to the server ui
  - `TODO: design request body JSON`

- response:

  - success:

        { "hash": "asd123" }, satus: 200

    - hash (str): the client's hash code

  - error:

        { "message": "..." }, status: 400 / 404

## `/result - POST`

- description:

  - client posts the result of the given task to this enpoint when completed

- request: 

 	  {
	      "hash": "asd123",
	      "result": {
              "train": { ... },
              "dev": { ... },
              "test": { ... }
          }
	  }

  - hash (str): the client's hash code
  - type (str): task type
  - `TODO: design result obj`

- response:

  - success:

        { "hash": "asd123" }, satus: 200

    - hash (str): the client's hash code

  - error:

        { "message": "..." }, status: 400 / 404
